% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_historical.R
\name{mdp_historical}
\alias{mdp_historical}
\title{mdp_historical}
\usage{
mdp_historical(transition, reward, discount, model_prior = NULL, state,
  action, model_names = NA, ...)
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{state}{sequence of states observed historically}

\item{action}{sequence of historical actions taken at time of observing that state}

\item{model_names}{optional vector of names for columns in model posterior distribution. 
Will be taken from names of transition list if none are provided here.}

\item{...}{additional arguments to \code{\link{mdp_compute_policy}}}
}
\value{
a list with component "df", a data.frame showing the historical state,
historical action, and what action would have been optimal by MDP; and a
data.frame showing the evolution of the belief over models during each subsequent observation
}
\description{
mdp_historical
}

