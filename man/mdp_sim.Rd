% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_sim.R
\name{mdp_sim}
\alias{mdp_sim}
\title{mdp sim}
\usage{
mdp_sim(transition, reward, discount, model_prior = NULL, x0, Tmax = 20,
  true_transition = transition, observation = NULL, a0 = 1,
  policy = NULL, max_iter = 500, epsilon = 1e-05,
  type = c("policy iteration", "value iteration", "finite time"))
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{x0}{initial state}

\item{Tmax}{length of time to simulate}

\item{true_transition}{actual transition used to drive simulation. If a fixed policy is given, then true_transition = transition
if it is not specified (i.e. avoids having to declare transition separately).}

\item{observation}{NULL by default, simulate perfect observations}

\item{a0}{previous action before starting, irrelivant unless actions influence observations and true_observation is not null}

\item{policy}{a vector of length n_obs, whose i'th entry is the index of the optimal action given the system is in (observed) state i.}

\item{max_iter}{maximum number of iterations to perform}

\item{epsilon}{convergence tolerance}

\item{type}{consider converged when policy converges or when value converges?}
}
\value{
a list, containing: data frame "df" with the state, action and a value at each time step in the simulation,
and an array "posterior", in which the t'th row shows the belief state at time t.
}
\description{
Simulate learning under the mdp policy
}
\details{
If no observation matrix is given, simulation is based on perfect observation.  Otherwise, simulation includes imperfect measurement
If a policy is given, this policy is used to determine the actions throughout the simulation and no learning takes place.
Otherwise, the simulation will compute the optimal (fully observed) MDP solution over the given transition models and model prior,
and continue to learn by updating the belief over models and the resulting optimal MDP policy.
}
\examples{
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]][["reward"]]

## example where true model is model 1
out <- mdp_sim(transition, reward, discount, x0 = 10,
                    Tmax = 20, true_transition = transition[[1]])
## Did we learn which one was the true model?
out$posterior[20,]

## simulate a fixed policy
df <- compute_mdp_policy(transition, reward, discount, model_prior = c(0.5, 0.5))
out <- mdp_sim(transition[[1]], reward, discount, x0 = 10,
               Tmax = 20,
               policy = df$policy)

## Simulate MDP strategy under observation uncertainty

out <- mdp_sim(transition = transition, reward, discount, x0 = 10,
               true_transition = transition[[1]],
               Tmax = 20, observation = models[[1]][["observation"]])
}

