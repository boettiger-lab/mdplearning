% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_learning.R
\name{mdp_learning}
\alias{mdp_learning}
\title{mdp learning}
\usage{
mdp_learning(transition, reward, discount, model_prior = NULL, x0,
  Tmax = 20, true_transition, observation = NULL, a0 = 1, ...)
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{x0}{initial state}

\item{Tmax}{termination time for finite time calculation, ignored otherwise}

\item{true_transition}{actual transition used to drive simulation.}

\item{observation}{NULL by default, simulate perfect observations}

\item{a0}{previous action before starting, irrelivant unless actions influence observations and true_observation is not null}

\item{...}{additional arguments to \code{\link{mdp_compute_policy}}}
}
\value{
a list, containing: data frame "df" with the state, action and a value at each time step in the simulation,
and an array "posterior", in which the t'th row shows the belief state at time t.
}
\description{
Simulate learning under the mdp policy
}
\examples{
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]]$reward

## example where true model is model 1
out <- mdp_learning(transition, reward, discount, x0 = 10,
                    Tmax = 20, true_transition = transition[[1]])
## Did we learn which one was the true model?
out$posterior[20,]

## Simulate MDP strategy under observation uncertainty
out <- mdp_learning(transition = transition, reward, discount, x0 = 10,
               true_transition = transition[[1]],
               Tmax = 20, observation = models[[1]]$observation)
}

