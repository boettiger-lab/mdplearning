% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_mdp_policy.R
\name{compute_mdp_policy}
\alias{compute_mdp_policy}
\title{compute mdp policy}
\usage{
compute_mdp_policy(transition, reward, discount, model_prior = NULL,
  max_iter = 500, epsilon = 1e-05, type = c("policy iteration",
  "value iteration", "finite time"))
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{max_iter}{maximum number of iterations to perform}

\item{epsilon}{convergence tolerance}

\item{type}{consider converged when policy converges or when value converges?}
}
\value{
a data.frame with the optimal policy and (discounted) value associated with each state
}
\description{
compute mdp policy
}
\examples{
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]][["reward"]]
df <- compute_mdp_policy(transition, reward, discount)
plot(df$state, df$state - df$policy, xlab = "stock", ylab="escapement")
}

