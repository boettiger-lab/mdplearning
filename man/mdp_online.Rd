% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_online.R
\name{mdp_online}
\alias{mdp_online}
\title{mdp online learning}
\usage{
mdp_online(transition, reward, discount, model_prior, prev_state, prev_action,
  state, ...)
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{prev_state}{the previous state of the system}

\item{prev_action}{the action taken after observing the previous state}

\item{state}{the most recent state observed}

\item{...}{additional arguments to \code{\link{mdp_compute_policy}}}
}
\value{
a list, with component 'action' giving the action recommended, and posterior, a 
vector of length(transition) giving the updated probability over models
}
\description{
Given previous state, previous action, and current state, update the model prior and propose best action
}
\details{
mdp_online provides a real-time updating mechanism given the latest observations.
 To learn the best model and compare proposed actions across historical data, use mdp_historical,
 which loops over mdp_online.  
 @examples 
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]]$reward
mdp_online(transition, reward, discount, c(0.5, 0.5), 10, 1, 12)
}
