% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_planning.R
\name{mdp_planning}
\alias{mdp_planning}
\title{mdp planning}
\usage{
mdp_planning(transition, reward, discount, model_prior = NULL, x0,
  Tmax = 20, observation = NULL, a0 = 1, policy, ...)
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{x0}{initial state}

\item{Tmax}{length of time to simulate}

\item{observation}{NULL by default, simulate perfect observations}

\item{a0}{previous action before starting, irrelivant unless actions influence observations and true_observation is not null}

\item{policy}{a vector of length n_obs, whose i'th entry is the index of the optimal action given the system is in (observed) state i.}

\item{...}{additional arguments to \code{\link{mdp_compute_policy}}}
}
\value{
a data frame "df" with the state, action and a value at each time step in the simulation
}
\description{
Simulate MDP under a given policy
}
\examples{
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]]$reward

df <- mdp_compute_policy(transition, reward, discount, model_prior = c(0.5, 0.5))
out <- mdp_planning(transition[[1]], reward, discount, x0 = 10,
               Tmax = 20, policy = df$policy)

## Simulate MDP strategy under observation uncertainty
out <- mdp_planning(transition[[1]], reward, discount, x0 = 10,
               Tmax = 20, policy = df$policy, 
               observation = models[[1]]$observation)

}
