% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_value_of_policy.R
\name{mdp_value_of_policy}
\alias{mdp_value_of_policy}
\title{mdp_value_of_policy}
\usage{
mdp_value_of_policy(policy, transition, reward, discount, model_prior = NULL,
  max_iter = 500, epsilon = 1e-05)
}
\arguments{
\item{policy}{the policy for which we want to determine the expected value}

\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{max_iter}{maximum number of iterations to perform}

\item{epsilon}{convergence tolerance}
}
\value{
the expected net present value of the given policy, for each state
}
\description{
Compute the expected net present (e.g. discounted) value of a (not-necessarily optimal) policy in a perfectly observed (MDP) system
}
\details{
transition a list of transition matrices
}
\examples{
source(system.file("examples/K_models.R", package="mdplearning"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]][["reward"]]
df <- mdp_compute_policy(transition, reward, discount)
v <- mdp_value_of_policy(df$policy, transition, reward, discount)
}
